{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOtQAHxHIEU3FhuZ2ujPr0p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seohyeonlee2020/audio-converter/blob/main/poetics_and_protocols_of_sampling_presentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual \"Sampling\": Audio To Generative Art"
      ],
      "metadata": {
        "id": "fPP4Q1mM_Wc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alex Lee"
      ],
      "metadata": {
        "id": "CaAwcCMRBxu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook hosts a model that takes audio files and outputs generative art. The audio file is converted to a spectrogram, which is then fed into a stable diffusion model that has tunable hyperparameters."
      ],
      "metadata": {
        "id": "P_Fl9GDVpJ5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Architecture (Under construction)\n",
        "\n",
        " - get audio file\n",
        " - extract spectrogram\n",
        " - covert spectrogram to image\n",
        " - set device (cuda) and generative pipeline\n",
        " - feed spectrogram to pipeline\n",
        " - show image and save with timestamp"
      ],
      "metadata": {
        "id": "jN00QayU7rCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Install dependencies ---\n",
        "!pip install torch torchaudio torchvision diffusers transformers accelerate safetensors pillow matplotlib --quiet"
      ],
      "metadata": {
        "id": "EwfWg1u68Onf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import io\n",
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torchvision.transforms as vtrans\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YNY_9fXx8z-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Upload any audio file ---\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "audio_path = list(uploaded.keys())[0]"
      ],
      "metadata": {
        "id": "Atb-llcL8217"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert non-wav files into wav if necessary\n",
        "# Check if the uploaded file is already a WAV\n",
        "if not audio_path.lower().endswith('.wav'):\n",
        "    print(f\"Converting '{audio_path}' to WAV format...\")\n",
        "    # Load the audio file\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "    # Define a new WAV file path\n",
        "    # Using os.path.splitext to get base name and then append .wav\n",
        "    base_name = os.path.splitext(audio_path)[0]\n",
        "    new_audio_path = f\"{base_name}.wav\"\n",
        "\n",
        "    # Save as WAV\n",
        "    torchaudio.save(new_audio_path, waveform, sample_rate)\n",
        "\n",
        "    # Update audio_path to point to the new WAV file\n",
        "    audio_path = new_audio_path\n",
        "    print(f\"Conversion complete. New audio path: '{audio_path}'\")\n",
        "else:\n",
        "    print(f\"File '{audio_path}' is already a WAV file. No conversion needed.\")"
      ],
      "metadata": {
        "id": "BXypombJA5M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejUQXkyW7mad"
      },
      "outputs": [],
      "source": [
        "# --- Define helper functions ---\n",
        "def get_spectrogram_image(audio_path):\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "    spec = T.Spectrogram(n_fft=512)(waveform)\n",
        "    if spec.shape[0] > 1:\n",
        "        spec = spec.mean(dim=0, keepdim=True)\n",
        "    spec_db = 10 * torch.log10(spec + 1e-9)\n",
        "    spec_db = (spec_db - spec_db.min()) / (spec_db.max() - spec_db.min())\n",
        "    spec_np = (spec_db.squeeze(0).numpy() * 255).astype(np.uint8)\n",
        "    rgb_img = Image.fromarray(spec_np).convert(\"RGB\")\n",
        "    return rgb_img.resize((400, 300))\n",
        "\n",
        "# --- Generate spectrogram image ---\n",
        "spec_image = get_spectrogram_image(audio_path)\n",
        "\n",
        "print(\"Normalized Spectrogram\")\n",
        "spec_image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup device and model ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(device)"
      ],
      "metadata": {
        "id": "JS-LY3UM_4VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prompt and generation ---\n",
        "prompt = (\n",
        "   \"caffeinated thoughts of a tired college student pulling their 2nd all nighter flying saucers and jellyfish living forever spicy pumpkin coffee abstract image no concrete objects\"\n",
        ")\n",
        "\n",
        "#strength: between 0 and 1. represents how much the original image is changed.\n",
        "#guidance_scale: how much influence the prompt has. works from 1 or higher. defaults to 7.5\n",
        "\n",
        "result = pipe(\n",
        "    prompt=prompt,\n",
        "    negative_prompt=\"no letters, no people, no recognizable objects\",\n",
        "    image=spec_image,\n",
        "    strength=0.9,\n",
        "    guidance_scale=2.5,\n",
        ").images\n",
        "\n",
        "# --- Save and show result ---\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "filename = f\"generative_art_from_{audio_path}_at_{timestamp}.png\"\n",
        "result[0].save(filename)\n",
        "\n",
        "print(f\"✅ Saved as {filename}\")\n",
        "display(result[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "pcmblLYh_5yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert image back into sound\n",
        "img = result[0].convert(\"RGB\")\n",
        "print(type(img))\n",
        "to_tensor = vtrans.ToTensor()\n",
        "rgb = to_tensor(img)  # [3, H, W]\n",
        "\n",
        "# Convert each channel to spectrogram magnitude\n",
        "def invert_channel(channel):\n",
        "    spec = torch.pow(10.0, (channel * 80 - 80) / 20.0)\n",
        "    freq_bins = channel.shape[0]\n",
        "    n_fft = (freq_bins - 1) * 2\n",
        "    hop_length = n_fft //2\n",
        "    griffin_lim = T.GriffinLim(n_fft=n_fft, hop_length=hop_length)\n",
        "    reconstructed_waveform = griffin_lim(spec)\n",
        "    return reconstructed_waveform\n",
        "\n",
        "waveforms = [invert_channel(c) for c in rgb]\n",
        "\n",
        "waveforms[0]\n"
      ],
      "metadata": {
        "id": "T-DXXhhJwkG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveforms[0].unsqueeze(0)"
      ],
      "metadata": {
        "id": "MD6oRcvK5PrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#method 1: take RGB channels apart and create separate audio\n",
        "for channel in rgb:\n",
        "    waveform = invert_channel(channel)\n",
        "    print(waveform.size())\n",
        "    max_val = waveform.abs().amax()\n",
        "    if max_val > 0:\n",
        "        waveform = waveform / max_val\n",
        "\n",
        "    print(waveform.unsqueeze(0).shape)\n",
        "    torchaudio.save(\"channel.wav\", waveform.unsqueeze(0), 24000)\n",
        "\n",
        "    from IPython.display import Audio\n",
        "\n",
        "    display(Audio(\"channel.wav\"))\n",
        "    break\n"
      ],
      "metadata": {
        "id": "EOb7JQ434ITN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#method 1: mix R, G, and B channels\n",
        "#I don't know much about mixing - anything I can do more here?\n",
        "# Stack all waveforms into a single tensor: [C, T]\n",
        "stacked = torch.stack(waveforms, dim=0)\n",
        "\n",
        "# Average across channels\n",
        "mixed = stacked.mean(dim=0)\n",
        "\n",
        "# Normalize safely\n",
        "max_val = mixed.abs().amax()\n",
        "if max_val > 0:\n",
        "    mixed = mixed / max_val\n",
        "\n",
        "import torchaudio.functional as F\n",
        "\n",
        "#make audio file longer\n",
        "def stretch_audio(original_audio, stretch_rate):\n",
        "    stretched = F.phase_vocoder(original_audio.unsqueeze(0), rate=1/stretch_rate, phase_advance=original_audio.size()[0] * stretch_rate)\n",
        "    return stretched\n",
        "\n",
        "stretched = stretch_audio(mixed, 3)\n",
        "\n",
        "torchaudio.save(\"color_inversion.wav\", stretched.to(torch.float32), 24000)\n",
        "\n",
        "\n",
        "\n",
        "# Play output audio\n",
        "Audio(\"color_inversion.wav\")"
      ],
      "metadata": {
        "id": "kllT6Jo50yv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#method 2:\n",
        "from scipy.signal import chirp\n",
        "\n",
        "img_np = np.array(result[0].resize((400, 300))) / 255.0\n",
        "height, width, _ = img_np.shape\n",
        "duration = 30  # seconds\n",
        "sample_rate = 24000\n",
        "samples = np.linspace(0, duration, int(sample_rate*duration))\n",
        "\n",
        "audio = np.zeros_like(samples)\n",
        "for x in range(width):\n",
        "    col = img_np[:, x, :].mean(axis=0)\n",
        "    freq = 800 * col[0]   # red → pitch\n",
        "    amp = 0.2 + 0.8 * col[1]    # green → amplitude\n",
        "    tone = amp * chirp(samples, f0=freq, f1=freq * 1.5, t1=duration, method='linear')\n",
        "    audio += tone / width\n",
        "\n",
        "# Convert NumPy array to PyTorch tensor and add a channel dimension\n",
        "audio_tensor = torch.from_numpy(audio).unsqueeze(0).to(torch.float32)\n",
        "\n",
        "torchaudio.save(\"color_inversion.wav\", audio_tensor, 24000)\n",
        "Audio(\"color_inversion.wav\")"
      ],
      "metadata": {
        "id": "e0umVMYe16Ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o4YKEUAOEKQu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}