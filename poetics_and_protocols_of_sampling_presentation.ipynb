{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNJk0A8cZ+PS0mvBRjaGA6W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seohyeonlee2020/audio-converter/blob/main/poetics_and_protocols_of_sampling_presentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visual Sampling: Audio To Generative Art"
      ],
      "metadata": {
        "id": "fPP4Q1mM_Wc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alex Lee"
      ],
      "metadata": {
        "id": "CaAwcCMRBxu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concept\n",
        "This project expands the concept of sampling, which originated from a musical practice where musicians mixed and matched \"samples\" of pre-existing music to create distinct results. This project extends that practice across mediums: audio is reinterpreted by Stable Diffusion into generative art, and then translated back to audio.\n",
        "Machine learning systems treat audio and images as interchangeable data—arrays that can be reshaped and reinterpreted. This system exploits that property to create a translation chain where unexpected meaning emerges through gaps in conversion.\n"
      ],
      "metadata": {
        "id": "IgwLGuC19ou_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Process:\n",
        "Audio → Spectrogram (visual representation)\n",
        "<br> Spectrogram → Abstract art (via Stable Diffusion)\n",
        "<br> Abstract art → Audio (via data conversion methods)\n",
        "\n",
        "<br> The same source material produces radically different results depending on conversion method. This variability is the point—it reveals how meaning is constructed through our methods of reading data."
      ],
      "metadata": {
        "id": "h1G_EZN390Pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is the source code for this project. You **DO NOT need to know any programming to use this system**. Please follow the instuctions carefully."
      ],
      "metadata": {
        "id": "2cSxapCq-A2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n",
        "\n",
        "Run the 2 code cells below by pressing the play button. This step might take up to 1 minute. Click 'Runtime' on the top and connect to a T4 GPU."
      ],
      "metadata": {
        "id": "xQcWA0F9-Wlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Install dependencies ---\n",
        "!pip install torch torchaudio torchvision diffusers transformers accelerate safetensors pillow matplotlib --quiet"
      ],
      "metadata": {
        "id": "EwfWg1u68Onf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import io\n",
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torchvision.transforms as vtrans\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YNY_9fXx8z-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio Upload\n",
        "\n",
        "Upload an audio file. You are free to upload any format as long as it is an audio file, but non-wav files will be converted to wav in the following cell."
      ],
      "metadata": {
        "id": "D9n42N-4-m_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Upload any audio file ---\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "audio_path = list(uploaded.keys())[0]"
      ],
      "metadata": {
        "id": "Atb-llcL8217"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert non-wav files into wav if necessary\n",
        "# Check if the uploaded file is already a WAV\n",
        "if not audio_path.lower().endswith('.wav'):\n",
        "    print(f\"Converting '{audio_path}' to WAV format...\")\n",
        "    # Load the audio file\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "    # Define a new WAV file path\n",
        "    # Using os.path.splitext to get base name and then append .wav\n",
        "    base_name = os.path.splitext(audio_path)[0]\n",
        "    new_audio_path = f\"{base_name}.wav\"\n",
        "\n",
        "    # Save as WAV\n",
        "    torchaudio.save(new_audio_path, waveform, sample_rate)\n",
        "\n",
        "    # Update audio_path to point to the new WAV file\n",
        "    audio_path = new_audio_path\n",
        "    print(f\"Conversion complete. New audio path: '{audio_path}'\")\n",
        "else:\n",
        "    print(f\"File '{audio_path}' is already a WAV file. No conversion needed.\")"
      ],
      "metadata": {
        "id": "BXypombJA5M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio -> Spectrogram"
      ],
      "metadata": {
        "id": "uGuhTnoqAHZD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejUQXkyW7mad"
      },
      "outputs": [],
      "source": [
        "# --- Define helper functions ---\n",
        "def get_spectrogram_image(audio_path):\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "    spec = T.Spectrogram(n_fft=512)(waveform)\n",
        "    if spec.shape[0] > 1:\n",
        "        spec = spec.mean(dim=0, keepdim=True)\n",
        "    spec_db = 10 * torch.log10(spec + 1e-9)\n",
        "    spec_db = (spec_db - spec_db.min()) / (spec_db.max() - spec_db.min())\n",
        "    spec_np = (spec_db.squeeze(0).numpy() * 255).astype(np.uint8)\n",
        "    rgb_img = Image.fromarray(spec_np).convert(\"RGB\")\n",
        "    return rgb_img.resize((400, 300))\n",
        "\n",
        "# --- Generate spectrogram image ---\n",
        "spec_image = get_spectrogram_image(audio_path)\n",
        "\n",
        "print(\"Normalized Spectrogram\")\n",
        "spec_image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup device and model ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(device)"
      ],
      "metadata": {
        "id": "JS-LY3UM_4VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spectrogram -> Image\n",
        "Parameters available for modification:\n",
        "\n",
        " - Stable Diffusion prompts (you can change the prompt in double quotation marks below)\n",
        " - Hyperparameters\n",
        "    - Guidance Scale: Determines how much influence the prompt has over the outcome. Works from 1 or higher. Defaults to 7.5\n",
        "    - Strength: Between 0 and 1. Determines how much the original image (ie spectrogram) is supposed to change.\n",
        "    - Negative Prompt: Things you don't want in the outcome.\n"
      ],
      "metadata": {
        "id": "7UXEdE9G_HJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prompt and generation ---\n",
        "prompt = (\n",
        "   \"caffeinated thoughts of a tired college student pulling their 2nd all nighter flying saucers and jellyfish living forever spicy pumpkin coffee abstract image no concrete objects\"\n",
        ")\n",
        "\n",
        "result = pipe(\n",
        "    prompt=prompt,\n",
        "    negative_prompt=\"no letters, no people, no recognizable objects\",\n",
        "    image=spec_image,\n",
        "    strength=0.9,\n",
        "    guidance_scale=2.5,\n",
        ").images\n",
        "\n",
        "# --- Save and show result ---\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "filename = f\"generative_art_from_{audio_path}_at_{timestamp}.png\"\n",
        "result[0].save(filename)\n",
        "\n",
        "print(f\"✅ Saved as {filename}\")\n",
        "display(result[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "pcmblLYh_5yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image -> Audio"
      ],
      "metadata": {
        "id": "-jgMwN6X_GCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convert image back into sound\n",
        "img = result[0].convert(\"RGB\")\n",
        "print(type(img))\n",
        "to_tensor = vtrans.ToTensor()\n",
        "rgb = to_tensor(img)  # [3, H, W]\n",
        "\n",
        "# Convert each channel to spectrogram magnitude\n",
        "def invert_channel(channel):\n",
        "    spec = torch.pow(10.0, (channel * 80 - 80) / 20.0)\n",
        "    freq_bins = channel.shape[0]\n",
        "    n_fft = (freq_bins - 1) * 2\n",
        "    hop_length = n_fft //2\n",
        "    griffin_lim = T.GriffinLim(n_fft=n_fft, hop_length=hop_length)\n",
        "    reconstructed_waveform = griffin_lim(spec)\n",
        "    return reconstructed_waveform\n",
        "\n",
        "waveforms = [invert_channel(c) for c in rgb]\n",
        "\n",
        "waveforms[0]\n"
      ],
      "metadata": {
        "id": "T-DXXhhJwkG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#method 1: take RGB channels apart and create separate audio\n",
        "for channel in rgb:\n",
        "    waveform = invert_channel(channel)\n",
        "    print(waveform.size())\n",
        "    max_val = waveform.abs().amax()\n",
        "    if max_val > 0:\n",
        "        waveform = waveform / max_val\n",
        "\n",
        "    print(waveform.unsqueeze(0).shape)\n",
        "    torchaudio.save(\"channel.wav\", waveform.unsqueeze(0), 24000)\n",
        "\n",
        "    from IPython.display import Audio\n",
        "\n",
        "    display(Audio(\"red_channel.wav\"))\n",
        "    break\n"
      ],
      "metadata": {
        "id": "EOb7JQ434ITN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mix R, G, and B channels\n",
        "# Stack all waveforms into a single tensor: [C, T]\n",
        "stacked = torch.stack(waveforms, dim=0)\n",
        "\n",
        "# Average across channels\n",
        "mixed = stacked.mean(dim=0)\n",
        "\n",
        "# Normalize safely\n",
        "max_val = mixed.abs().amax()\n",
        "if max_val > 0:\n",
        "    mixed = mixed / max_val\n",
        "\n",
        "import torchaudio.functional as F\n",
        "\n",
        "#make audio file longer\n",
        "def stretch_audio(original_audio, stretch_rate):\n",
        "    stretched = F.phase_vocoder(original_audio.unsqueeze(0), rate=1/stretch_rate, phase_advance=original_audio.size()[0] * stretch_rate)\n",
        "    return stretched\n",
        "\n",
        "stretched = stretch_audio(mixed, 3)\n",
        "\n",
        "torchaudio.save(\"color_inversion.wav\", stretched.to(torch.float32), 24000)\n",
        "\n",
        "# Play output audio\n",
        "Audio(\"color_inversion.wav\")"
      ],
      "metadata": {
        "id": "kllT6Jo50yv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#method 2:\n",
        "from scipy.signal import chirp\n",
        "\n",
        "img_np = np.array(result[0].resize((400, 300))) / 255.0\n",
        "height, width, _ = img_np.shape\n",
        "duration = 30  # seconds\n",
        "sample_rate = 24000\n",
        "samples = np.linspace(0, duration, int(sample_rate*duration))\n",
        "\n",
        "audio = np.zeros_like(samples)\n",
        "for x in range(width):\n",
        "    col = img_np[:, x, :].mean(axis=0)\n",
        "    freq = 800 * col[0]   # red → pitch\n",
        "    amp = 0.2 + 0.8 * col[1]    # green → amplitude\n",
        "    tone = amp * chirp(samples, f0=freq, f1=freq * 1.5, t1=duration, method='linear')\n",
        "    audio += tone / width\n",
        "\n",
        "# Convert NumPy array to PyTorch tensor and add a channel dimension\n",
        "audio_tensor = torch.from_numpy(audio).unsqueeze(0).to(torch.float32)\n",
        "\n",
        "torchaudio.save(\"chirp_file.wav\", audio_tensor, 24000)\n",
        "\n",
        "#Play audio\n",
        "Audio(\"chirp_file.wav\")"
      ],
      "metadata": {
        "id": "e0umVMYe16Ng"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}